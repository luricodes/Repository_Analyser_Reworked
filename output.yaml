repo_analyzer:
  main.py:
    type: text
    content: "# repo_analyzer/main.py\n\nfrom repo_analyzer.core.application import run\n\n__all__ = [\"run\"]\n\nif __name__ == \"__main__\":\n    run()\n"
    size: 137
    created: 1729084327.0167124
    modified: 1729200537.5516126
    permissions: 0o100666
  traversal:
    patterns.py:
      type: text
      content: "# repo_analyzer/traversal/patterns.py\n\nimport fnmatch\nimport logging\nimport re\nfrom functools import lru_cache\nfrom typing import Pattern, Sequence\n\nfrom colorama import Fore, Style\n\n@lru_cache(maxsize=None)\ndef compile_regex(pattern: str) -> Pattern:\n    \"\"\"\n    Kompiliert ein Regex-Muster und cached es.\n\n    Args:\n        pattern (str): Das Regex-Muster als String.\n\n    Returns:\n        Pattern: Das kompilierte Regex-Muster.\n    \"\"\"\n    return re.compile(pattern)\n\ndef matches_patterns(filename: str, patterns: Sequence[str]) -> bool:\n    \"\"\"\n    Prüft, ob der Dateiname einem der Muster entspricht (Glob oder Regex).\n\n    Args:\n        filename (str): Der Name der Datei.\n        patterns (Sequence[str]): Eine Sequenz von Mustern (Glob oder Regex).\n\n    Returns:\n        bool: True, wenn der Dateiname einem der Muster entspricht, sonst False.\n    \"\"\"\n    for pattern in patterns:\n        if pattern.startswith('regex:'):\n            regex: str = pattern[len('regex:'):]\n            try:\n                compiled: Pattern = compile_regex(regex)\n                if compiled.match(filename):\n                    return True\n            except re.error as e:\n                logging.error(\n                    f\"{Fore.RED}Ungültiges Regex-Muster '{regex}': {e}{Style.RESET_ALL}\"\n                )\n        else:\n            if fnmatch.fnmatch(filename, pattern):\n                return True\n    return False\n"
      size: 1463
      created: 1729084326.9668045
      modified: 1729342627.186228
      permissions: 0o100666
    traverser.py:
      type: text
      content: "# repo_analyzer/traversal/traverser.py\n\nimport logging\nfrom pathlib import Path\nfrom typing import Any, Dict, List, Set, Tuple, Optional, Generator\nfrom concurrent.futures import ThreadPoolExecutor, as_completed, Future\nfrom tqdm import tqdm\n\nfrom repo_analyzer.processing.file_processor import process_file\nfrom repo_analyzer.traversal.patterns import matches_patterns\nfrom colorama import Fore, Style\n\nfrom repo_analyzer.core.flags import shutdown_event\n\ndef traverse_and_collect(\n    root_dir: Path,\n    excluded_folders: Set[str],\n    excluded_files: Set[str],\n    exclude_patterns: List[str],\n    follow_symlinks: bool\n) -> Tuple[List[Path], int, int]:\n    paths: List[Path] = []\n    included = 0\n    excluded = 0\n    visited_paths: Set[Path] = set()\n\n    stack = [root_dir]\n\n    while stack:\n        if shutdown_event.is_set():\n            logging.info(\"Traversal aborted due to shutdown event.\")\n            break\n\n        current_dir = stack.pop()\n        try:\n            if follow_symlinks:\n                resolved_dir = current_dir.resolve()\n                if resolved_dir in visited_paths:\n                    logging.warning(\n                        f\"{Fore.RED}Zirkulärer symbolischer Link gefunden: {current_dir}{Style.RESET_ALL}\"\n                    )\n                    continue\n                visited_paths.add(resolved_dir)\n        except Exception as e:\n            logging.error(\n                f\"{Fore.RED}Fehler beim Auflösen von {current_dir}: {e}{Style.RESET_ALL}\"\n            )\n            continue\n\n        try:\n            for entry in current_dir.iterdir():\n                if shutdown_event.is_set():\n                    logging.info(\"Traversal aborted due to shutdown event.\")\n                    break\n\n                if entry.is_dir():\n                    if (\n                        entry.name in excluded_folders\n                        or matches_patterns(entry.name, exclude_patterns)\n                    ):\n                        logging.debug(\n                            f\"{Fore.CYAN}Ausschließen von Ordner: {entry}{Style.RESET_ALL}\"\n                        )\n                        continue\n                    stack.append(entry)\n                elif entry.is_file():\n                    if (\n                        entry.name in excluded_files\n                        or matches_patterns(entry.name, exclude_patterns)\n                    ):\n                        logging.debug(\n                            f\"{Fore.YELLOW}Ausschließen von Datei: {entry}{Style.RESET_ALL}\"\n                        )\n                        excluded += 1\n                        continue\n                    paths.append(entry)\n                    included += 1\n        except PermissionError as e:\n            logging.warning(\n                f\"{Fore.YELLOW}Konnte Verzeichnis nicht lesen: {current_dir} - {e}{Style.RESET_ALL}\"\n            )\n        except Exception as e:\n            logging.error(\n                f\"{Fore.RED}Fehler beim Durchlaufen von {current_dir}: {e}{Style.RESET_ALL}\"\n            )\n\n    return paths, included, excluded\n\ndef get_directory_structure(\n    root_dir: Path,\n    max_file_size: int,\n    include_binary: bool,\n    excluded_folders: Set[str],\n    excluded_files: Set[str],\n    follow_symlinks: bool,\n    image_extensions: Set[str],\n    exclude_patterns: List[str],\n    threads: int,\n    encoding: str = 'utf-8',\n    hash_algorithm: Optional[str] = \"md5\",\n) -> Tuple[Dict[str, Any], Dict[str, Any]]:\n\n    dir_structure: Dict[str, Any] = {}\n\n    files_to_process, included_files, excluded_files_count = traverse_and_collect(\n        root_dir,\n        excluded_folders,\n        excluded_files,\n        exclude_patterns,\n        follow_symlinks\n    )\n    total_files: int = included_files + excluded_files_count\n    excluded_percentage: float = (excluded_files_count / total_files * 100) if total_files else 0.0\n\n    logging.info(f\"Gesamtzahl der Dateien: {total_files}\")\n    logging.info(f\"\
        Ausgeschlossene Dateien: {excluded_files_count} ({excluded_percentage:.2f}%)\")\n    logging.info(f\"Verarbeitete Dateien: {included_files}\")\n    \n    pbar: tqdm = tqdm(\n        total=included_files,\n        desc=\"Verarbeite Dateien\",\n        unit=\"file\",\n        dynamic_ncols=True\n    )\n\n    failed_files: List[Dict[str, str]] = []\n\n    with ThreadPoolExecutor(max_workers=threads) as executor:\n        future_to_file: Dict[Future[Tuple[str, Any]], Path] = {}\n        try:\n            for file_path in files_to_process:\n                if shutdown_event.is_set():\n                    break  # Abbruch bei gesetztem Flag\n                future = executor.submit(\n                    process_file,\n                    file_path,\n                    max_file_size,\n                    include_binary,\n                    image_extensions,\n                    encoding=encoding,\n                    hash_algorithm=hash_algorithm,\n                )\n                future_to_file[future] = file_path\n        except KeyboardInterrupt:\n            logging.warning(\"\\nAbbruch durch Benutzer. Versuche, laufende Aufgaben zu beenden...\")\n            executor.shutdown(wait=False, cancel_futures=True)\n            pbar.close()\n            raise\n        except Exception as e:\n            logging.error(f\"Unerwarteter Fehler während des Einreichens von Aufgaben: {e}\")\n            executor.shutdown(wait=False, cancel_futures=True)\n            pbar.close()\n            raise\n\n        try:\n            for future in as_completed(future_to_file):\n                if shutdown_event.is_set():\n                    break  # Abbruch bei gesetztem Flag\n                file_path: Path = future_to_file[future]\n                try:\n                    filename, file_info = future.result()\n                    if file_info is not None:\n                        try:\n                            relative_parent: Path = file_path.parent.relative_to(root_dir)\n                        except ValueError:\n                            relative_parent = file_path.parent\n\n                        current: Dict[str, Any] = dir_structure\n                        for part in relative_parent.parts:\n                            current = current.setdefault(part, {})\n                        current[filename] = file_info\n                except Exception as e:\n                    try:\n                        relative_parent: Path = file_path.parent.relative_to(root_dir)\n                    except ValueError:\n                        relative_parent = file_path.parent\n\n                    current: Dict[str, Any] = dir_structure\n                    for part in relative_parent.parts:\n                        current = current.setdefault(part, {})\n                    current[\n                        file_path.name\n                    ] = {\n                        \"type\": \"error\",\n                        \"content\": f\"Fehler bei der Verarbeitung: {str(e)}\"\n                    }\n                    logging.error(f\"Fehler beim Verarbeiten der Datei {file_path}: {e}\")\n                    failed_files.append(\n                        {\"file\": str(file_path), \"error\": str(e)}\n                    )\n                finally:\n                    pbar.update(1)\n        except KeyboardInterrupt:\n            logging.warning(\"\\nAbbruch durch Benutzer während der Verarbeitung. Versuche, laufende Aufgaben zu beenden...\")\n            executor.shutdown(wait=False, cancel_futures=True)\n            pbar.close()\n            raise\n\n    pbar.close()\n\n    summary: Dict[str, Any] = {\n        \"total_files\": total_files,\n        \"excluded_files\": excluded_files_count,\n        \"included_files\": included_files,\n        \"excluded_percentage\": excluded_percentage,\n        \"failed_files\": failed_files\n    }\n\n    if hash_algorithm is not None:\n        summary[\"hash_algorithm\"] = hash_algorithm\n\n    logging.info(\"Zusammenfassung:\")\n    logging.info(f\"  Verarbeitete Dateien: {included_files}\")\n\
        \    logging.info(f\"  Ausgeschlossene Dateien: {excluded_files_count} ({excluded_percentage:.2f}%)\")\n    logging.info(f\"  Fehlgeschlagene Dateien: {len(failed_files)}\")\n    if hash_algorithm is not None:\n        logging.info(f\"  Verwendeter Hash-Algorithmus: {hash_algorithm}\")\n\n    return dir_structure, summary\n\ndef get_directory_structure_stream(\n    root_dir: Path,\n    max_file_size: int,\n    include_binary: bool,\n    excluded_folders: Set[str],\n    excluded_files: Set[str],\n    follow_symlinks: bool,\n    image_extensions: Set[str],\n    exclude_patterns: List[str],\n    threads: int,\n    encoding: str = 'utf-8',\n    hash_algorithm: Optional[str] = \"md5\",\n) -> Generator[Dict[str, Any], None, None]:\n    files_to_process, included_files, excluded_files_count = traverse_and_collect(\n        root_dir,\n        excluded_folders,\n        excluded_files,\n        exclude_patterns,\n        follow_symlinks\n    )\n    total_files: int = included_files + excluded_files_count\n    excluded_percentage: float = (excluded_files_count / total_files * 100) if total_files else 0.0\n\n    logging.info(f\"Gesamtzahl der Dateien: {total_files}\")\n    logging.info(f\"Ausgeschlossene Dateien: {excluded_files_count} ({excluded_percentage:.2f}%)\")\n    logging.info(f\"Verarbeitete Dateien: {included_files}\")\n    \n    pbar: tqdm = tqdm(\n        total=included_files,\n        desc=\"Verarbeite Dateien\",\n        unit=\"file\",\n        dynamic_ncols=True\n    )\n\n    failed_files: List[Dict[str, str]] = []\n\n    with ThreadPoolExecutor(max_workers=threads) as executor:\n        future_to_file: Dict[Future[Tuple[str, Any]], Path] = {}\n        for file_path in files_to_process:\n            if shutdown_event.is_set():\n                break  # Abbruch bei gesetztem Flag\n            future = executor.submit(\n                process_file,\n                file_path,\n                max_file_size,\n                include_binary,\n                image_extensions,\n                encoding=encoding,\n                hash_algorithm=hash_algorithm,\n            )\n            future_to_file[future] = file_path\n\n        try:\n            for future in as_completed(future_to_file):\n                if shutdown_event.is_set():\n                    break  # Abbruch bei gesetztem Flag\n                file_path: Path = future_to_file[future]\n                try:\n                    filename, file_info = future.result()\n                    if file_info is not None:\n                        try:\n                            relative_parent: Path = file_path.parent.relative_to(root_dir)\n                        except ValueError:\n                            relative_parent = file_path.parent\n\n                        yield {\n                            \"parent\": str(relative_parent),\n                            \"filename\": filename,\n                            \"info\": file_info\n                        }\n                except Exception as e:\n                    logging.error(f\"Fehler beim Verarbeiten der Datei {file_path}: {e}\")\n                    yield {\n                        \"parent\": str(file_path.parent.relative_to(root_dir)) if root_dir in file_path.parent.resolve().parents else str(file_path.parent),\n                        \"filename\": file_path.name,\n                        \"info\": {\n                            \"type\": \"error\",\n                            \"content\": f\"Fehler beim Verarbeiten der Datei: {str(e)}\",\n                            \"exception_type\": type(e).__name__,\n                            \"exception_message\": str(e)\n                        }\n                    }\n                finally:\n                    pbar.update(1)\n        except KeyboardInterrupt:\n            logging.warning(\"\\nAbbruch durch Benutzer während der Verarbeitung. Versuche, laufende Aufgaben zu beenden...\")\n            executor.shutdown(wait=False, cancel_futures=True)\n            pbar.close()\n            raise\n\n    pbar.close()\n\n    # Zusammenfassung\n  \
        \  summary: Dict[str, Any] = {\n        \"total_files\": total_files,\n        \"excluded_files\": excluded_files_count,\n        \"included_files\": included_files,\n        \"excluded_percentage\": excluded_percentage,\n        \"failed_files\": failed_files\n    }\n\n    if hash_algorithm is not None:\n        summary[\"hash_algorithm\"] = hash_algorithm\n\n    yield {\n        \"summary\": summary\n    }\n"
      size: 12564
      created: 1729084326.9668045
      modified: 1729369528.0341153
      permissions: 0o100666
    __init__.py:
      type: excluded
      reason: binary_or_image
  utils:
    helpers.py:
      type: text
      content: "# repo_analyzer/utils/helpers.py\n\nimport logging\nfrom colorama import Fore, Style\nfrom pathlib import Path\n\n\ndef is_binary_alternative(file_path: Path) -> bool:\n    \"\"\"\n    Alternative Methode zur Binärdatei-Erkennung:\n    Prüft, ob die Datei NULL-Bytes enthält.\n    \"\"\"\n    try:\n        with open(file_path, 'rb') as f:\n            chunk = f.read(1024)\n            if b'\\0' in chunk:\n                return True\n        return False\n    except Exception as e:\n        logging.error(f\"{Fore.RED}Fehler bei der alternativen Binärprüfung für {file_path}: {e}{Style.RESET_ALL}\")\n        return False\n"
      size: 624
      created: 1729084326.9269679
      modified: 1729194494.7420895
      permissions: 0o100666
    mime_type.py:
      type: text
      content: "# repo_analyzer/utils/mime_type.py\n\nimport magic\nimport threading\nfrom colorama import Fore, Style\nimport logging\nfrom pathlib import Path\nfrom .helpers import is_binary_alternative\n\nthread_local_data = threading.local()\n\ndef get_magic_instance():\n    if not hasattr(thread_local_data, 'mime'):\n        try:\n            thread_local_data.mime = magic.Magic(mime=True)\n        except Exception as e:\n            logging.error(f\"{Fore.RED}Fehler beim Initialisieren von magic: {e}{Style.RESET_ALL}\")\n            thread_local_data.mime = None\n    return thread_local_data.mime\n\ndef is_binary(file_path: Path) -> bool:\n    \"\"\"\n    Prüft, ob eine Datei binär ist, basierend auf dem MIME-Typ.\n    Falls die MIME-Erkennung fehlschlägt, verwendet sie eine alternative Methode.\n    \"\"\"\n    try:\n        mime = get_magic_instance()\n        if mime is None:\n            return is_binary_alternative(file_path)\n        \n        # Lese nur die ersten 8192 Bytes der Datei\n        with open(file_path, 'rb') as f:\n            file_content = f.read(8192)\n        \n        mime_type = mime.from_buffer(file_content)\n        logging.debug(f\"Datei: {file_path} - MIME-Typ: {mime_type}\")\n        return not mime_type.startswith('text/')\n    except Exception as e:\n        logging.warning(f\"{Fore.YELLOW}Fehler bei der Erkennung des MIME-Typs für {file_path}: {e}{Style.RESET_ALL}\")\n        # Alternative Methode zur Binärprüfung\n        return is_binary_alternative(file_path)\n"
      size: 1505
      created: 1729084326.9269679
      modified: 1729341109.1362557
      permissions: 0o100666
    __init__.py:
      type: excluded
      reason: binary_or_image
  __init__.py:
    type: excluded
    reason: binary_or_image
  processing:
    file_processor.py:
      type: text
      content: "# repo_analyzer/processing/file_processor.py\n\nimport base64\nimport logging\nimport os\nfrom pathlib import Path\nfrom typing import Any, Dict, Optional, Set, Tuple, Union\n\nfrom ..cache.sqlite_cache import get_cached_entry, set_cached_entry, get_connection_context\nfrom ..processing.hashing import compute_file_hash\nfrom ..utils.mime_type import is_binary\n\n# Create a module-specific logger\nlogger = logging.getLogger(__name__)\n\ndef process_file(\n    file_path: Path,\n    max_file_size: int,\n    include_binary: bool,\n    image_extensions: Set[str],\n    encoding: str = 'utf-8',\n    hash_algorithm: Optional[str] = \"md5\",\n) -> Tuple[str, Optional[Dict[str, Any]]]:\n    filename = file_path.name\n\n    try:\n        stat = file_path.stat()\n        current_size = stat.st_size\n        current_mtime = stat.st_mtime\n    except OSError as e:\n        logger.error(f\"Failed to get file stats for {file_path}: {e}\")\n        return filename, {\n            \"type\": \"error\",\n            \"content\": f\"Failed to get file stats: {str(e)}\",\n            \"exception_type\": type(e).__name__,\n            \"exception_message\": str(e)\n        }\n\n    if current_size > max_file_size:\n        logger.info(f\"File too large and will be excluded: {file_path} ({current_size} bytes)\")\n        return filename, {\n            \"type\": \"excluded\",\n            \"reason\": \"file_size\",\n            \"size\": current_size\n        }\n\n    file_hash = None\n    file_info = None\n\n    # Check cache\n    if hash_algorithm is not None:\n        cached_entry = _check_cache(file_path, current_size, current_mtime, hash_algorithm)\n        if cached_entry:\n            return filename, cached_entry\n\n    # Compute hash if needed\n    if hash_algorithm is not None:\n        file_hash = _compute_hash(file_path, hash_algorithm)\n        if isinstance(file_hash, dict) and file_hash.get(\"type\") == \"error\":\n            return filename, file_hash\n\n    # Process file content\n    file_info = _process_file_content(file_path, include_binary, image_extensions, max_file_size, encoding)\n    if file_info.get(\"type\") == \"error\" or file_info.get(\"type\") == \"excluded\":\n        return filename, file_info\n\n    # Add metadata\n    _add_metadata(file_info, stat)\n\n    # Update cache\n    if hash_algorithm is not None and file_hash is not None:\n        _update_cache(file_path, file_hash, hash_algorithm, file_info, current_size, current_mtime)\n\n    return filename, file_info\n\ndef _check_cache(file_path: Path, current_size: int, current_mtime: float, hash_algorithm: str) -> Optional[Dict[str, Any]]:\n    with get_connection_context() as conn:\n        cached_entry = get_cached_entry(conn, str(file_path.resolve()))\n\n    if cached_entry:\n        cached_size = cached_entry.get(\"size\")\n        cached_mtime = cached_entry.get(\"mtime\")\n        cached_algorithm = cached_entry.get(\"hash_algorithm\")\n\n        if (\n            cached_size == current_size and\n            cached_mtime == current_mtime and\n            cached_algorithm == hash_algorithm\n        ):\n            logger.debug(f\"Cache hit for file: {file_path}\")\n            return cached_entry.get(\"file_info\")\n\n    return None\n\ndef _compute_hash(file_path: Path, hash_algorithm: str) -> Union[str, Dict[str, Any]]:\n    try:\n        return compute_file_hash(file_path, algorithm=hash_algorithm)\n    except Exception as e:\n        logger.error(f\"Failed to compute hash for {file_path}: {e}\")\n        return {\n            \"type\": \"error\",\n            \"content\": f\"Failed to compute hash: {str(e)}\",\n            \"exception_type\": type(e).__name__,\n            \"exception_message\": str(e)\n        }\n\ndef _process_file_content(file_path: Path, include_binary: bool, image_extensions: Set[str], max_file_size: int, encoding: str) -> Dict[str, Any]:\n    file_extension = file_path.suffix.lower()\n    is_image = file_extension in image_extensions\n\n    try:\n        binary = is_binary(file_path)\n\n        if (binary or is_image)\
        \ and not include_binary:\n            logger.debug(f\"Excluding {'binary' if binary else 'image'} file: {file_path}\")\n            return {\n                \"type\": \"excluded\",\n                \"reason\": \"binary_or_image\"\n            }\n\n        if binary:\n            return _read_binary_file(file_path, max_file_size)\n        else:\n            return _read_text_file(file_path, max_file_size, encoding)\n\n    except PermissionError as e:\n        logger.error(f\"Permission denied when reading file: {file_path}\")\n        return {\n            \"type\": \"error\",\n            \"content\": f\"Permission denied: {str(e)}\",\n            \"exception_type\": type(e).__name__,\n            \"exception_message\": str(e)\n        }\n    except IsADirectoryError:\n        logger.error(f\"Attempted to process a directory as a file: {file_path}\")\n        return {\n            \"type\": \"error\",\n            \"content\": \"Is a directory\"\n        }\n    except OSError as e:\n        logger.error(f\"OS error when processing file {file_path}: {e}\")\n        return {\n            \"type\": \"error\",\n            \"content\": f\"OS error: {str(e)}\",\n            \"exception_type\": type(e).__name__,\n            \"exception_message\": str(e)\n        }\n    except Exception as e:\n        logger.error(f\"Unexpected error when processing file {file_path}: {e}\")\n        return {\n            \"type\": \"error\",\n            \"content\": f\"Unexpected error: {str(e)}\",\n            \"exception_type\": type(e).__name__,\n            \"exception_message\": str(e)\n        }\n\ndef _read_binary_file(file_path: Path, max_file_size: int) -> Dict[str, Any]:\n    try:\n        file_size = file_path.stat().st_size\n        if file_size > max_file_size:\n            logger.info(f\"Binary file too large to include: {file_path} ({file_size} bytes)\")\n            return {\n                \"type\": \"excluded\",\n                \"reason\": \"binary_too_large\",\n                \"size\": file_size\n            }\n        \n        with open(file_path, 'rb') as f:\n            content = base64.b64encode(f.read()).decode('utf-8')\n        logger.debug(f\"Included binary file: {file_path}\")\n        return {\n            \"type\": \"binary\",\n            \"content\": content\n        }\n    except Exception as e:\n        logger.error(f\"Error reading binary file {file_path}: {e}\")\n        return {\n            \"type\": \"error\",\n            \"content\": f\"Failed to read binary file: {str(e)}\",\n            \"exception_type\": type(e).__name__,\n            \"exception_message\": str(e)\n        }\n\ndef _read_text_file(file_path: Path, max_file_size: int, encoding: str) -> Dict[str, Any]:\n    try:\n        with open(file_path, 'r', encoding=encoding) as f:\n            content = f.read(max_file_size)\n        logger.debug(f\"Read text file: {file_path}\")\n        return {\n            \"type\": \"text\",\n            \"content\": content\n        }\n    except UnicodeDecodeError:\n        logger.warning(f\"UnicodeDecodeError for file {file_path}. Falling back to binary read.\")\n        return _read_binary_file(file_path, max_file_size)\n    except Exception as e:\n        logger.error(f\"Error reading text file {file_path}: {e}\")\n        return {\n            \"type\": \"error\",\n            \"content\": f\"Failed to read text file: {str(e)}\",\n            \"exception_type\": type(e).__name__,\n            \"exception_message\": str(e)\n        }\n\ndef _add_metadata(file_info: Dict[str, Any], stat: os.stat_result) -> None:\n    try:\n        file_info.update({\n            \"size\": stat.st_size,\n            \"created\": getattr(stat, 'st_birthtime', None),\n            \"modified\": stat.st_mtime,\n            \"permissions\": oct(stat.st_mode)\n        })\n    except Exception as e:\n        logger.warning(f\"Could not retrieve complete metadata: {e}\")\n\ndef _update_cache(file_path: Path, file_hash: str, hash_algorithm: str, file_info: Dict[str, Any], current_size: int, current_mtime: float) ->\
        \ None:\n    with get_connection_context() as conn:\n        set_cached_entry(\n            conn,\n            str(file_path.resolve()),\n            file_hash,\n            hash_algorithm,\n            file_info,\n            current_size,\n            current_mtime\n        )\n"
      size: 8258
      created: 1729084326.9554887
      modified: 1729366034.1167178
      permissions: 0o100666
    hashing.py:
      type: text
      content: "# repo_analyzer/processing/hashing.py\n\nimport hashlib\nimport logging\nfrom pathlib import Path\nfrom typing import Optional\n\n# Optional: Farbliche Hervorhebung in Logs beibehalten\n# Falls Farben nicht benötigt werden, können die folgenden Zeilen entfernt werden\nfrom colorama import Fore, Style, init\n\n# Initialisierung von colorama\ninit(autoreset=True)\n\n\ndef compute_file_hash(file_path: Path, algorithm: str = \"sha256\") -> Optional[str]:\n    \"\"\"Berechnet den Hash einer Datei basierend auf dem angegebenen Algorithmus.\n\n    Args:\n        file_path (Path): Der Pfad zur Datei.\n        algorithm (str, optional): Der Hash-Algorithmus (z.B. 'md5', 'sha1', 'sha256'). \n                                   Standard ist 'sha256'.\n\n    Returns:\n        Optional[str]: Der Hash der Datei als Hex-String oder None bei Fehlern.\n    \"\"\"\n    if not algorithm:\n        logging.error(\"Kein Hash-Algorithmus angegeben.\")\n        return None\n\n    algorithm = algorithm.lower()\n    try:\n        hasher = hashlib.new(algorithm)\n    except ValueError:\n        logging.error(f\"{Fore.RED}Ungültiger Hash-Algorithmus: {algorithm}{Style.RESET_ALL}\")\n        return None\n\n    try:\n        with file_path.open('rb') as file:\n            for chunk in iter(lambda: file.read(65536), b\"\"):\n                hasher.update(chunk)\n        return hasher.hexdigest()\n    except FileNotFoundError:\n        logging.warning(f\"{Fore.YELLOW}Datei nicht gefunden: {file_path}{Style.RESET_ALL}\")\n    except PermissionError:\n        logging.warning(f\"{Fore.YELLOW}Keine Berechtigung zum Lesen der Datei: {file_path}{Style.RESET_ALL}\")\n    except OSError as e:\n        logging.warning(f\"{Fore.YELLOW}OS-Fehler beim Lesen der Datei {file_path}: {e}{Style.RESET_ALL}\")\n    \n    return None\n"
      size: 1797
      created: 1729084326.9585054
      modified: 1729349228.268538
      permissions: 0o100666
    __init__.py:
      type: excluded
      reason: binary_or_image
  output:
    __init__.py:
      type: text
      content: '# repo_analyzer/output/__init__.py'
      size: 34
      created: 1729084326.8967838
      modified: 1729342710.4181423
      permissions: 0o100666
    ndjson_output.py:
      type: text
      content: "# repo_analyzer/output/ndjson_output.py\n\nimport json\nimport logging\nfrom typing import Any, Dict, Generator\n\nfrom colorama import Fore, Style\n\ndef output_to_ndjson(data_generator: Generator[Dict[str, Any], None, None], output_file: str) -> None:\n    \"\"\"\n    Schreibt die Daten in eine NDJSON-Datei (Newline Delimited JSON).\n\n    Args:\n        data_generator (Generator[Dict[str, Any], None, None]): Ein Generator, der die zu schreibenden Daten liefert.\n        output_file (str): Der Pfad zur Ausgabedatei.\n    \"\"\"\n    try:\n        with open(output_file, 'w', encoding='utf-8') as out_file:\n            for data in data_generator:\n                json_line = json.dumps(data, ensure_ascii=False)\n                out_file.write(json_line + '\\n')\n    except Exception as e:\n        logging.error(\n            f\"{Fore.RED}Fehler beim Schreiben der NDJSON-Ausgabedatei: {e}{Style.RESET_ALL}\"\n        )\n"
      size: 923
      created: 1729354108.0696104
      modified: 1729355430.3192868
      permissions: 0o100666
    xml_output.py:
      type: text
      content: "# repo_analyzer/output/xml_output.py\n\nimport logging\nimport re\nfrom typing import Any, Dict\nfrom xml.etree.ElementTree import Element, SubElement, ElementTree, tostring\nfrom xml.dom import minidom\n\nfrom colorama import Fore, Style\n\ndef sanitize_tag(tag: str) -> str:\n    \"\"\"\n    Sanitizes a string to be used as an XML tag.\n    \"\"\"\n    # Replace spaces and invalid characters with underscores\n    tag = re.sub(r'\\s+', '_', tag)\n    tag = re.sub(r'[^\\w\\-\\.]', '', tag)\n    # Ensure the tag doesn't start with a number\n    if re.match(r'^\\d', tag):\n        tag = f'item_{tag}'\n    return tag\n\ndef dict_to_xml(parent: Element, data: Any) -> None:\n    \"\"\"\n    Recursively converts a dictionary to XML elements, distinguishing between directories and files.\n    \"\"\"\n    if isinstance(data, dict):\n        for key, value in data.items():\n            if isinstance(value, dict) and 'type' in value:\n                # Datei\n                file_element = SubElement(parent, 'file', name=key)\n                for file_key, file_value in value.items():\n                    if file_key != 'name' and file_key != 'type':\n                        child = SubElement(file_element, sanitize_tag(file_key))\n                        if isinstance(file_value, dict) or isinstance(file_value, list):\n                            dict_to_xml(child, file_value)\n                        else:\n                            child.text = str(file_value)\n                    elif file_key == 'type':\n                        child = SubElement(file_element, sanitize_tag(file_key))\n                        child.text = str(file_value)\n            elif isinstance(value, dict):\n                # Ordner\n                dir_element = SubElement(parent, 'directory', name=key)\n                dict_to_xml(dir_element, value)\n            elif isinstance(value, list):\n                # Falls eine Liste vorhanden ist, z.B. für mehrere Dateien oder Ordner\n                for item in value:\n                    if isinstance(item, dict):\n                        dict_to_xml(parent, item)\n            else:\n                # Andere Typen können als Text hinzugefügt werden\n                child = SubElement(parent, sanitize_tag(key))\n                child.text = str(value)\n    elif isinstance(data, list):\n        for item in data:\n            dict_to_xml(parent, item)\n    else:\n        parent.text = str(data)\n\ndef prettify_xml(element: Element) -> str:\n    \"\"\"\n    Returns a pretty-printed XML string for the Element.\n    \"\"\"\n    rough_string = tostring(element, 'utf-8')\n    try:\n        reparsed = minidom.parseString(rough_string)\n        return reparsed.toprettyxml(indent=\"  \")\n    except Exception as e:\n        logging.error(f\"{Fore.RED}Fehler beim Formatieren der XML: {e}{Style.RESET_ALL}\")\n        return rough_string.decode('utf-8')\n\ndef output_to_xml(data: Dict[str, Any], output_file: str) -> None:\n    \"\"\"\n    Writes the data to an XML file with improved structure and readability.\n    \"\"\"\n    try:\n        logging.debug(f\"Beginne XML-Konvertierung mit Daten: {data}\")\n        root = Element('repository')\n        dict_to_xml(root, data)\n\n        pretty_xml = prettify_xml(root)\n        logging.debug(f\"XML-Struktur nach Konvertierung: {pretty_xml[:200]}...\")  # Loggt die ersten 200 Zeichen\n\n        with open(output_file, 'w', encoding='utf-8') as xml_file:\n            xml_file.write(pretty_xml)\n        \n        logging.info(f\"XML-Ausgabe erfolgreich in '{output_file}' geschrieben.\")\n    except Exception as e:\n        logging.error(\n            f\"{Fore.RED}Fehler beim Konvertieren der Daten zu XML: {e}{Style.RESET_ALL}\"\n        )\n"
      size: 3715
      created: 1729084326.9053328
      modified: 1729369312.276447
      permissions: 0o100666
    json_output.py:
      type: text
      content: "# repo_analyzer/output/json_output.py\n\nimport json\nimport logging\nimport os\nfrom typing import Any, Dict, Generator\n\nfrom colorama import Fore, Style\n\nclass JSONStreamWriter:\n    \"\"\"\n    Kontextmanager für das inkrementelle Schreiben einer JSON-Datei.\n    Sichert, dass die JSON-Struktur korrekt abgeschlossen wird, auch bei Unterbrechungen.\n    \"\"\"\n\n    def __init__(self, output_file: str):\n        self.output_file = output_file\n        self.file = None\n        self.first_entry = True\n\n    def __enter__(self):\n        self.file = open(self.output_file, 'w', encoding='utf-8')\n        self.file.write('{\\n')\n        self.file.write('  \"structure\": [\\n')  # Changed to list for streaming\n        return self\n\n    def write_entry(self, data: Dict[str, Any]) -> None:\n        if not self.first_entry:\n            self.file.write(',\\n')\n        else:\n            self.first_entry = False\n        json.dump(data, self.file, ensure_ascii=False, indent=4)\n\n    def write_summary(self, summary: Dict[str, Any]) -> None:\n        self.file.write('\\n  ],\\n')\n        self.file.write('  \"summary\": ')\n        json.dump(summary, self.file, ensure_ascii=False, indent=4)\n        self.file.write('\\n')\n        self.file.write('}\\n')\n\n    def __exit__(self, exc_type, exc_val, exc_tb):\n        if self.file:\n            if exc_type is not None:\n                # If an exception occurred, close the JSON structure gracefully\n                try:\n                    self.file.write('\\n  ],\\n')\n                    self.file.write('  \"summary\": {}\\n')\n                    self.file.write('}\\n')\n                except Exception as e:\n                    logging.error(\n                        f\"{Fore.RED}Fehler beim Abschließen der JSON-Struktur: {e}{Style.RESET_ALL}\"\n                    )\n            self.file.close()\n\ndef output_to_json(data: Dict[str, Any], output_file: str) -> None:\n    \"\"\"\n    Schreibt die Daten in eine JSON-Datei im Standardmodus.\n\n    Args:\n        data (Dict[str, Any]): Die Daten, die in die JSON-Datei geschrieben werden sollen.\n        output_file (str): Der Pfad zur Ausgabedatei.\n    \"\"\"\n    try:\n        with open(output_file, 'w', encoding='utf-8') as out_file:\n            json.dump(data, out_file, ensure_ascii=False, indent=4)\n    except Exception as e:\n        logging.error(\n            f\"{Fore.RED}Fehler beim Schreiben der JSON-Ausgabedatei: {e}{Style.RESET_ALL}\"\n        )\n\ndef output_to_json_stream(data_generator: Generator[Dict[str, Any], None, None], output_file: str) -> None:\n    \"\"\"\n    Schreibt die Daten in eine JSON-Datei im Streaming-Modus.\n\n    Args:\n        data_generator (Generator[Dict[str, Any], None, None]): Ein Generator, der die zu schreibenden Daten liefert.\n        output_file (str): Der Pfad zur Ausgabedatei.\n    \"\"\"\n    try:\n        with JSONStreamWriter(output_file) as writer:\n            summary = {}\n            for data in data_generator:\n                if \"summary\" in data:\n                    summary = data[\"summary\"]\n                    continue\n                parent = data[\"parent\"]\n                filename = data[\"filename\"]\n                info = data[\"info\"]\n\n                # Prepare the JSON entry\n                file_path = os.path.join(parent, filename) if parent else filename\n                file_entry = {\n                    \"path\": file_path.replace(os.sep, '/'),\n                    \"info\": info\n                }\n\n                writer.write_entry(file_entry)\n\n            # Write the summary at the end\n            if summary:\n                writer.write_summary(summary)\n            else:\n                writer.write_summary({})\n    except Exception as e:\n        logging.error(\n            f\"{Fore.RED}Fehler beim Schreiben der JSON-Ausgabedatei im Streaming-Modus: {e}{Style.RESET_ALL}\"\n        )\n"
      size: 3903
      created: 1729084326.901302
      modified: 1729366449.0199065
      permissions: 0o100666
    output_factory.py:
      type: text
      content: "# repo_analyzer/output/output_factory.py\n\nfrom typing import Callable, Dict, Any\n\nfrom .json_output import output_to_json, output_to_json_stream\nfrom .yaml_output import output_to_yaml\nfrom .xml_output import output_to_xml\nfrom .ndjson_output import output_to_ndjson\n\nclass OutputFactory:\n    \"\"\"\n    Factory zur Auswahl des Ausgabeformats basierend auf dem Benutzerinput.\n    \"\"\"\n\n    _output_methods: Dict[str, Callable[..., None]] = {\n        \"json\": output_to_json,\n        \"json_stream\": output_to_json_stream,\n        \"yaml\": output_to_yaml,\n        \"xml\": output_to_xml,\n        \"ndjson\": output_to_ndjson,\n    }\n\n    @classmethod\n    def get_output(cls, format: str, streaming: bool = False) -> Callable[..., None]:\n        \"\"\"\n        Gibt die entsprechende Ausgabe-Methode zurück.\n\n        Args:\n            format (str): Das gewünschte Ausgabeformat.\n            streaming (bool): Ob Streaming unterstützt werden soll.\n\n        Returns:\n            Callable[..., None]: Die Ausgabe-Methode.\n        \"\"\"\n        try:\n            if streaming and format == \"json\":\n                return cls._output_methods[\"json_stream\"]\n            return cls._output_methods[format]\n        except KeyError:\n            available = ', '.join(cls._output_methods.keys())\n            raise ValueError(f\"Unbekanntes Ausgabeformat: {format}. Verfügbare Formate sind: {available}.\")\n"
      size: 1419
      created: 1729194494.7209425
      modified: 1729370130.147702
      permissions: 0o100666
    yaml_output.py:
      type: text
      content: "# repo_analyzer/output/yaml_output.py\n\nimport logging\nfrom typing import Any, Dict\nfrom pathlib import Path\nimport tempfile\nimport shutil\n\nfrom colorama import Fore, Style\nimport yaml\n\n\nclass YAMLError(Exception):\n    \"\"\"Custom exception for YAML-related errors.\"\"\"\n    pass\n\n\ndef output_to_yaml(data: Dict[str, Any], output_file: str, **yaml_options) -> None:\n    \"\"\"\n    Schreibt die Daten in eine YAML-Datei mit erweiterten Optionen, atomaren Schreibvorgängen und verbesserter Fehlerbehandlung.\n\n    Args:\n        data (Dict[str, Any]): Die Daten, die in die YAML-Datei geschrieben werden sollen.\n        output_file (str): Der Pfad zur Ausgabedatei.\n        **yaml_options: Zusätzliche Optionen für das YAML-Dump, z.B. default_flow_style.\n    \"\"\"\n    default_options = {\n        'allow_unicode': True,\n        'sort_keys': False,\n        'default_flow_style': False,  # Verbessert die Lesbarkeit durch Block-Stil\n        'width': 4096,  # Erhöht die maximale Zeilenbreite, um lange Listen zu vermeiden\n    }\n    # Update default options mit benutzerdefinierten Optionen, falls vorhanden\n    dump_options = {**default_options, **yaml_options}\n\n    try:\n        # Validierung der Daten\n        validate_data(data)\n\n        # Atomare Schreiboperation: Schreiben in eine temporäre Datei und Umbenennen\n        temp_dir = Path(output_file).parent\n        with tempfile.NamedTemporaryFile('w', delete=False, dir=temp_dir, encoding='utf-8') as temp_file:\n            yaml.dump(data, temp_file, **dump_options)\n            temp_file_path = Path(temp_file.name)\n\n        # Umbenennen der temporären Datei zur endgültigen Ausgabedatei\n        shutil.move(str(temp_file_path), output_file)\n\n        logging.info(f\"YAML-Ausgabe erfolgreich in '{output_file}' geschrieben.\")\n    except yaml.YAMLError as e:\n        logging.error(\n            f\"{Fore.RED}YAML-Fehler beim Dumpen der Daten in '{output_file}': {e}{Style.RESET_ALL}\"\n        )\n        raise YAMLError(f\"YAML-Fehler: {e}\") from e\n    except (IOError, OSError) as e:\n        logging.error(\n            f\"{Fore.RED}Fehler beim Schreiben der YAML-Ausgabedatei '{output_file}': {e}{Style.RESET_ALL}\"\n        )\n        raise\n    except Exception as e:\n        logging.error(\n            f\"{Fore.RED}Unerwarteter Fehler beim Schreiben der YAML-Ausgabedatei '{output_file}': {e}{Style.RESET_ALL}\"\n        )\n        raise\n\n\ndef validate_data(data: Any) -> None:\n    \"\"\"\n    Validiert, ob die Daten YAML-kompatibel sind.\n\n    Args:\n        data (Any): Die zu validierenden Daten.\n\n    Raises:\n        YAMLError: Wenn die Daten nicht YAML-kompatibel sind.\n    \"\"\"\n    try:\n        # Versucht, die Daten zu serialisieren, ohne sie zu speichern\n        yaml.safe_dump(data)  # Korrekt ohne Dumper-Parameter\n    except yaml.YAMLError as e:\n        raise YAMLError(f\"Daten sind nicht YAML-kompatibel: {e}\") from e\n"
      size: 2941
      created: 1729084326.901302
      modified: 1729370116.5190623
      permissions: 0o100666
  core:
    flags.py:
      type: text
      content: '# repo_analyzer/core/flags.py


        import threading


        # Thread-sicheres Event zur Signalisierung eines Shutdowns

        shutdown_event = threading.Event()

        '
      size: 149
      created: 1729363561.9854374
      modified: 1729363564.9788864
      permissions: 0o100666
    summary.py:
      type: text
      content: "# repo_analyzer/core/summary.py\n\nfrom typing import Any, Dict, Optional\n\n\ndef create_summary(\n    structure: Dict[str, Any],\n    summary: Dict[str, Any],\n    include_summary: bool,\n    hash_algorithm: Optional[str] = None\n) -> Dict[str, Any]:\n    \"\"\"\n    Creates a summary of the directory structure.\n\n    Args:\n        structure (Dict[str, Any]): The directory structure.\n        summary (Dict[str, Any]): The existing summary.\n        include_summary (bool): Flag indicating whether to include the summary.\n        hash_algorithm (Optional[str], optional): The hash algorithm used or None.\n\n    Returns:\n        Dict[str, Any]: The final data structure for output.\n    \"\"\"\n    output_data: Dict[str, Any] = {}\n\n    if include_summary and summary:\n        output_data[\"summary\"] = summary\n        if hash_algorithm:\n            output_data[\"hash_algorithm\"] = hash_algorithm\n\n    output_data[\"structure\"] = structure\n\n    return output_data\n"
      size: 975
      created: 1729194494.7209425
      modified: 1729249534.3728385
      permissions: 0o100666
    __init__.py:
      type: text
      content: '# repo_analyzer/core/__init__.py


        from .application import run

        '
      size: 66
      created: 1729194494.7209425
      modified: 1729194494.7209425
      permissions: 0o100666
    application.py:
      type: text
      content: "# repo_analyzer/core/application.py\n\nimport logging\nimport signal\nimport multiprocessing\nimport sys\nfrom pathlib import Path\nfrom typing import Any, Dict, Optional, Set, List\n\nfrom repo_analyzer.cache.sqlite_cache import (\n    clean_cache,\n    close_all_connections,\n    get_connection_context,\n    initialize_connection_pool,\n)\nfrom repo_analyzer.cli.parser import get_default_cache_path, parse_arguments\nfrom repo_analyzer.config.config import Config\nfrom repo_analyzer.config.defaults import (\n    CACHE_DB_FILE,\n    DEFAULT_EXCLUDED_FILES,\n    DEFAULT_EXCLUDED_FOLDERS,\n    DEFAULT_MAX_FILE_SIZE_MB\n)\nfrom repo_analyzer.logging.setup import setup_logging\nfrom repo_analyzer.output.output_factory import OutputFactory\nfrom repo_analyzer.traversal.traverser import get_directory_structure, get_directory_structure_stream\nfrom colorama import init as colorama_init\n\nfrom .flags import shutdown_event  # Import aus dem neuen Modul\n\nDEFAULT_THREAD_MULTIPLIER = 2\n\ndef signal_handler(sig, frame):\n    if not shutdown_event.is_set():\n        logging.warning(\"Programm durch Benutzer unterbrochen (STRG+C).\")\n        shutdown_event.set()\n    else:\n        logging.warning(\"Zweites STRG+C erkannt. Sofortiger Abbruch.\")\n        sys.exit(1)\n\ndef initialize_cache_directory(cache_path: Path) -> Path:\n    try:\n        cache_path.mkdir(parents=True, exist_ok=True)\n        logging.debug(f\"Cache-Verzeichnis erstellt oder existiert bereits: {cache_path}\")\n    except OSError as e:\n        logging.error(f\"Fehler beim Erstellen des Cache-Verzeichnisses '{cache_path}': {e}\")\n        sys.exit(1)\n    return cache_path\n\ndef run() -> None:\n    colorama_init(autoreset=True)\n    args = parse_arguments()\n\n    # Registriere den globalen Signal-Handler\n    signal.signal(signal.SIGINT, signal_handler)\n    signal.signal(signal.SIGTERM, signal_handler)\n\n    config_manager = Config()\n    try:\n        config_manager.load(args.config)\n    except FileNotFoundError:\n        logging.error(f\"Konfigurationsdatei nicht gefunden: {args.config}\")\n        sys.exit(1)\n    except Exception as e:\n        logging.error(f\"Fehler beim Laden der Konfigurationsdatei: {e}\")\n        sys.exit(1)\n    config = config_manager.data\n\n    setup_logging(args.verbose, args.log_file)\n\n    root_directory: Path = Path(args.root_directory).resolve()\n    output_file: str = args.output\n    include_binary: bool = args.include_binary\n    additional_excluded_folders: Set[str] = set(args.exclude_folders)\n    additional_excluded_files: Set[str] = set(args.exclude_files)\n    follow_symlinks: bool = args.follow_symlinks\n    additional_image_extensions: Set[str] = {\n        ext.lower() if ext.startswith('.') else f'.{ext.lower()}'\n        for ext in args.image_extensions\n    }\n    include_summary: bool = args.include_summary\n    output_format: str = args.format\n    threads: Optional[int] = args.threads\n    exclude_patterns: List[str] = args.exclude_patterns\n    encoding: str = args.encoding\n    cache_path: Path = Path(args.cache_path).expanduser().resolve()\n    pool_size: int = args.pool_size\n\n    if args.no_hash:\n        hash_algorithm = None\n        logging.info(\"Hash-Verifizierung ist deaktiviert.\")\n    else:\n        hash_algorithm = args.hash_algorithm\n        logging.info(f\"Verwende Hash-Algorithmus: {hash_algorithm}\")\n\n    if threads is None:\n        threads = multiprocessing.cpu_count() * DEFAULT_THREAD_MULTIPLIER\n        logging.info(f\"Dynamisch festgelegte Anzahl der Threads: {threads}\")\n\n    try:\n        max_file_size = config_manager.get_max_size(cli_max_size=args.max_size)\n        logging.info(f\"Maximale Dateigröße zum Lesen: {max_file_size / (1024 * 1024)} MB\")\n    except ValueError as ve:\n        logging.error(f\"Fehler bei der Bestimmung der maximalen Dateigröße: {ve}\")\n        sys.exit(1)\n\n    config_excluded_folders: Set[str] = set(config.get('exclude_folders', []))\n    config_excluded_files: Set[str] = set(config.get('exclude_files', []))\n    config_exclude_patterns:\
        \ List[str] = config.get('exclude_patterns', [])\n\n    excluded_folders: Set[str] = (\n        DEFAULT_EXCLUDED_FOLDERS\n        .union(additional_excluded_folders, config_excluded_folders)\n    )\n    excluded_files: Set[str] = (\n        set(DEFAULT_EXCLUDED_FILES)\n        .union(additional_excluded_files, config_excluded_files)\n    )\n    exclude_patterns: List[str] = exclude_patterns + config_exclude_patterns\n\n    image_extensions: Set[str] = {\n        '.png',\n        '.jpg',\n        '.jpeg',\n        '.gif',\n        '.bmp',\n        '.svg',\n        '.webp',\n        '.tiff',\n    }.union(additional_image_extensions)\n\n    logging.info(f\"Durchsuche das Verzeichnis: {root_directory}\")\n    logging.info(f\"Ausgeschlossene Ordner: {', '.join(sorted(excluded_folders))}\")\n    logging.info(f\"Ausgeschlossene Dateien: {', '.join(sorted(excluded_files))}\")\n    if not include_binary:\n        logging.info(\"Binäre Dateien und Bilddateien sind ausgeschlossen.\")\n    else:\n        logging.info(\"Binäre Dateien und Bilddateien werden einbezogen.\")\n    logging.info(f\"Ausgabe in: {output_file} ({output_format})\")\n    logging.info(\n        f\"Symbolische Links werden {'gefolgt' if follow_symlinks else 'nicht gefolgt'}\"\n    )\n    logging.info(f\"Bilddateiendungen: {', '.join(sorted(image_extensions))}\")\n    logging.info(f\"Ausschlussmuster: {', '.join(exclude_patterns)}\")\n    logging.info(f\"Anzahl der Threads: {threads}\")\n    logging.info(f\"Standard-Encoding: {encoding}\")\n    logging.info(f\"Cache-Pfad: {cache_path}\")\n\n    cache_dir: Path = initialize_cache_directory(cache_path)\n    cache_db_path: Path = cache_dir / CACHE_DB_FILE\n    db_path_str: str = str(cache_db_path)\n    try:\n        initialize_connection_pool(db_path_str, pool_size=pool_size)\n    except Exception as e:\n        logging.error(f\"Fehler beim Initialisieren des Verbindungspools: {e}\")\n        sys.exit(1)\n\n    try:\n        clean_cache(root_directory)\n    except Exception as e:\n        logging.error(f\"Fehler beim Bereinigen des Caches: {e}\")\n        sys.exit(1)\n\n    try:\n        if args.stream:\n            # Streaming-Modus verwenden\n            if output_format in [\"json\", \"ndjson\"]:\n                # JSON-Streaming oder NDJSON-Output verwenden\n                data_gen = get_directory_structure_stream(\n                    root_dir=root_directory,\n                    max_file_size=max_file_size,\n                    include_binary=include_binary,\n                    excluded_folders=excluded_folders,\n                    excluded_files=excluded_files,\n                    follow_symlinks=follow_symlinks,\n                    image_extensions=image_extensions,\n                    exclude_patterns=exclude_patterns,\n                    threads=threads,\n                    encoding=encoding,\n                    hash_algorithm=hash_algorithm,\n                )\n                OutputFactory.get_output(output_format, streaming=True)(data_gen, output_file)\n            else:\n                logging.error(\"Streaming-Modus ist nur für JSON und NDJSON verfügbar.\")\n                sys.exit(1)\n        else:\n            # Standardmodus verwenden\n            structure, summary = get_directory_structure(\n                root_dir=root_directory,\n                max_file_size=max_file_size,\n                include_binary=include_binary,\n                excluded_folders=excluded_folders,\n                excluded_files=excluded_files,\n                follow_symlinks=follow_symlinks,\n                image_extensions=image_extensions,\n                exclude_patterns=exclude_patterns,\n                threads=threads,\n                encoding=encoding,\n                hash_algorithm=hash_algorithm,\n            )\n            # Generiere die zusammengefassten Daten\n            output_data: Dict[str, Any] = {\n                \"summary\": summary,\n                \"structure\": structure\n            } if include_summary else structure\n            OutputFactory.get_output(output_format)(output_data,\
        \ output_file)\n\n        logging.info(\n            f\"Der aktuelle Stand der Ordnerstruktur\"\n            f\"{' und die Zusammenfassung ' if include_summary else ''}\"\n            f\"wurden in '{output_file}' gespeichert.\"\n        )\n    except KeyboardInterrupt:\n        if shutdown_event.is_set():\n            logging.warning(\"Erzwungener Programmabbruch.\")\n        else:\n            logging.warning(\"Programm durch Benutzer unterbrochen (STRG+C).\")\n        sys.exit(1)\n    except (OSError, IOError) as e:\n        logging.error(\n            f\"Fehler beim Schreiben der Ausgabedatei nach Abbruch: {str(e)}\"\n        )\n        sys.exit(1)\n    except ValueError as ve:\n        logging.error(f\"Fehler beim Auswählen des Ausgabeformats: {ve}\")\n        sys.exit(1)\n    finally:\n        try:\n            close_all_connections()\n        except Exception as e:\n            logging.error(f\"Fehler beim Schließen der Verbindungen: {e}\")\n"
      size: 9089
      created: 1729194494.7209425
      modified: 1729370177.168944
      permissions: 0o100666
  config:
    config.py:
      type: text
      content: "# repo_analyzer/config/config.py\n\nimport json\nimport logging\nimport threading\nfrom pathlib import Path\nfrom typing import Any, Dict, Optional, Literal\n\nimport yaml\nfrom colorama import Fore, Style\n\nfrom .defaults import (\n    CACHE_DB_FILE,\n    DEFAULT_EXCLUDED_FILES,\n    DEFAULT_EXCLUDED_FOLDERS,\n    DEFAULT_MAX_FILE_SIZE_MB\n)\nfrom .loader import load_config\n\n\n# Konstanten für unterstützte Dateiformate\nSUPPORTED_FORMATS: tuple[Literal['.yaml', '.yml', '.json'], ...] = (\n    '.yaml',\n    '.yml',\n    '.json',\n)\n\n\ndef log_error(message: str) -> None:\n    \"\"\"\n    Hilfsfunktion zum Loggen von Fehlern mit roten Farben.\n\n    Args:\n        message (str): Die Fehlermeldung.\n    \"\"\"\n    logging.error(f\"{Fore.RED}{message}{Style.RESET_ALL}\")\n\n\nclass Config:\n    \"\"\"\n    Singleton-Klasse zur Verwaltung der Konfiguration.\n    \"\"\"\n\n    _instance: Optional['Config'] = None\n    _lock: threading.Lock = threading.Lock()\n\n    data: Dict[str, Any]\n\n    def __new__(cls) -> 'Config':\n        if cls._instance is None:\n            with cls._lock:\n                if cls._instance is None:\n                    cls._instance = super(Config, cls).__new__(cls)\n        return cls._instance\n\n    def __init__(self) -> None:\n        \"\"\"\n        Initialisiert die Konfigurationsdaten, falls noch nicht geschehen.\n        \"\"\"\n        if not hasattr(self, 'data'):\n            self.data = {}\n\n    def load(self, config_path: Optional[str]) -> None:\n        \"\"\"\n        Lädt die Konfigurationsdatei, falls angegeben.\n\n        Args:\n            config_path (Optional[str]): Der Pfad zur Konfigurationsdatei.\n        \"\"\"\n        if config_path:\n            try:\n                loaded_config = load_config(config_path)\n                if isinstance(loaded_config, dict):\n                    self.data.update(loaded_config)\n                else:\n                    raise TypeError(\"Geladene Konfiguration muss ein Wörterbuch sein.\")\n            except Exception as e:\n                log_error(f\"Fehler beim Laden der Konfigurationsdatei: {e}\")\n\n    def get_max_size(self, cli_max_size: Optional[int]) -> int:\n        \"\"\"\n        Bestimmt die maximale Dateigröße basierend auf CLI-Argumenten, Konfiguration und Standardwerten.\n\n        Args:\n            cli_max_size (Optional[int]): Der vom Benutzer angegebene Wert für max_size in MB.\n\n        Returns:\n            int: Die zu verwendende maximale Dateigröße in Bytes.\n\n        Raises:\n            ValueError: Wenn der angegebene Wert ungültig ist.\n        \"\"\"\n        if cli_max_size is not None:\n            if cli_max_size <= 0:\n                raise ValueError(\"Die maximale Dateigröße muss positiv sein.\")\n            return cli_max_size * 1024 * 1024  # Umwandlung von MB in Bytes\n\n        config_max_size = self.data.get('max_size')\n        if config_max_size is not None:\n            if isinstance(config_max_size, int) and config_max_size > 0:\n                return config_max_size * 1024 * 1024  # Umwandlung von MB in Bytes\n            else:\n                raise ValueError(\n                    \"Der Wert für 'max_size' in der Konfigurationsdatei ist ungültig. \"\n                    \"Bitte geben Sie einen positiven ganzzahligen Wert in MB an.\"\n                )\n\n        return DEFAULT_MAX_FILE_SIZE_MB * 1024 * 1024  # Standardwert in Bytes\n\n    def save(self, config_path: str) -> None:\n        \"\"\"\n        Speichert die aktuelle Konfiguration in eine Datei (YAML oder JSON).\n\n        Args:\n            config_path (str): Der Pfad zur Konfigurationsdatei.\n        \"\"\"\n        config_file = Path(config_path)\n        suffix = config_file.suffix.lower()\n\n        if suffix not in SUPPORTED_FORMATS:\n            log_error(f\"Unbekanntes Konfigurationsdateiformat zum Speichern: {config_path}\")\n            return\n\n        try:\n            with config_file.open('w', encoding='utf-8') as file:\n                if suffix in ('.yaml', '.yml'):\n                    yaml.dump(self.data,\
        \ file, allow_unicode=True, sort_keys=False)\n                elif suffix == '.json':\n                    json.dump(self.data, file, ensure_ascii=False, indent=4)\n        except Exception as e:\n            log_error(f\"Fehler beim Speichern der Konfigurationsdatei: {e}\")\n"
      size: 4339
      created: 1729194494.7209425
      modified: 1729264722.2555754
      permissions: 0o100666
    loader.py:
      type: text
      content: "# repo_analyzer/config/loader.py\n\nimport json\nimport logging\nfrom pathlib import Path\nfrom typing import Any, Dict, Optional\n\nimport yaml\nfrom colorama import Fore, Style\n\n# Konstanten für unterstützte Dateiendungen\nSUPPORTED_EXTENSIONS = {'.yaml', '.yml', '.json'}\n\n\ndef log_error(message: str) -> None:\n    \"\"\"\n    Protokolliert eine Fehlermeldung in roter Farbe.\n\n    Args:\n        message (str): Die Fehlermeldung.\n    \"\"\"\n    logging.error(f\"{Fore.RED}{message}{Style.RESET_ALL}\")\n\n\ndef load_config(config_path: Optional[str]) -> Dict[str, Any]:\n    \"\"\"\n    Lädt eine Konfigurationsdatei (YAML oder JSON).\n\n    Args:\n        config_path (Optional[str]): Der Pfad zur Konfigurationsdatei.\n\n    Returns:\n        Dict[str, Any]: Die geladene Konfiguration als Dictionary.\n                        Gibt ein leeres Dictionary zurück, wenn kein Pfad angegeben ist\n                        oder ein Fehler beim Laden auftritt.\n    \"\"\"\n    if not config_path:\n        return {}\n\n    config_file = Path(config_path)\n    if not config_file.is_file():\n        log_error(f\"Die Konfigurationsdatei existiert nicht oder ist kein reguläres Datei: {config_path}\")\n        return {}\n\n    try:\n        with config_file.open('r', encoding='utf-8') as file:\n            file_suffix = config_file.suffix.lower()\n            if file_suffix in ('.yaml', '.yml'):\n                config = yaml.safe_load(file) or {}\n            elif file_suffix == '.json':\n                config = json.load(file)\n            else:\n                log_error(f\"Unbekanntes Konfigurationsdateiformat: {config_path}\")\n                return {}\n    except (yaml.YAMLError, json.JSONDecodeError) as parse_err:\n        log_error(f\"Fehler beim Parsen der Konfigurationsdatei: {parse_err}\")\n        return {}\n    except IOError as io_err:\n        log_error(f\"IO-Fehler beim Laden der Konfigurationsdatei: {io_err}\")\n        return {}\n    except Exception as e:\n        log_error(f\"Unerwarteter Fehler beim Laden der Konfigurationsdatei: {e}\")\n        return {}\n\n    # Validierung der Konfigurationsparameter\n    config = validate_config(config)\n\n    return config\n\n\ndef validate_config(config: Dict[str, Any]) -> Dict[str, Any]:\n    \"\"\"\n    Validiert die geladenen Konfigurationsparameter.\n\n    Args:\n        config (Dict[str, Any]): Das geladene Konfigurations-Dictionary.\n\n    Returns:\n        Dict[str, Any]: Das validierte Konfigurations-Dictionary.\n                        Ungültige Einträge werden entfernt.\n    \"\"\"\n    # Validierung des 'max_size' Parameters\n    max_size = config.get('max_size')\n    if max_size is not None:\n        if not isinstance(max_size, int) or max_size <= 0:\n            log_error(f\"Ungültiger Wert für 'max_size' in der Konfigurationsdatei: {max_size}\")\n            config.pop('max_size')  # Entfernen ungültiger Werte\n\n    # Weitere Validierungen können hier hinzugefügt werden\n    # Beispiel:\n    # log_level = config.get('log_level')\n    # if log_level not in {'DEBUG', 'INFO', 'WARNING', 'ERROR', 'CRITICAL'}:\n    #     log_error(f\"Ungültiger Wert für 'log_level': {log_level}\")\n    #     config.pop('log_level')\n\n    return config\n"
      size: 3235
      created: 1729084326.9427845
      modified: 1729249384.1597111
      permissions: 0o100666
    defaults.py:
      type: text
      content: "# repo_analyzer/config/defaults.py\n\nDEFAULT_EXCLUDED_FOLDERS = {\n    'tmp',\n    'node_modules',\n    '.git',\n    'dist',\n    'build',\n    'out',\n    'target',\n    'public',\n    'cache',\n    'temp',\n    'coverage',\n    'test-results',\n    'reports',\n    '.vscode',\n    '.idea',\n    'logs',\n    'assets',\n    'bower_components',\n    '.next',\n    'venv',\n    'tests',\n    'repo_analyzer.egg-info',\n    '__pycache__',\n    '.mypy_cache'\n\n}\n\nDEFAULT_EXCLUDED_FILES = {\n    'config.json',\n    'secret.txt',\n    'package-lock.json',\n    'favicon.ico',\n    'GeistMonoVF.woff',\n    'GeistVF.woff',\n    '.repo_structure_cache',\n}\n\nDEFAULT_MAX_FILE_SIZE_MB = 50  #Megabyte\n\nCACHE_DB_FILE = '.repo_structure_cache.db'\n"
      size: 747
      created: 1729186253.9660797
      modified: 1729369404.1520767
      permissions: 0o100666
    __init__.py:
      type: excluded
      reason: binary_or_image
  gui:
    controllers.py:
      type: excluded
      reason: binary_or_image
    main_window.py:
      type: excluded
      reason: binary_or_image
    dialogs.py:
      type: excluded
      reason: binary_or_image
    widgets.py:
      type: excluded
      reason: binary_or_image
    __init__.py:
      type: excluded
      reason: binary_or_image
  cli:
    parser.py:
      type: text
      content: "# repo_analyzer/cli/parser.py\n\nimport argparse\nimport os\nfrom pathlib import Path\n\ndef get_default_cache_path() -> str:\n    \"\"\"\n    Gibt den Standardpfad für das Cache-Verzeichnis zurück.\n    \"\"\"\n    home = Path.home()\n    return str(home / \"Documents\" / \"Datenbank\") if os.name == 'nt' else str(home / \".repo_analyzer\" / \"cache\")\n\ndef parse_arguments():\n    \"\"\"\n    Parst die Kommandozeilenargumente und gibt die konfigurierten Argumente zurück.\n    \"\"\"\n    parser = argparse.ArgumentParser(\n        description=(\n            \"Listet ein Repository in einer JSON-, YAML-, XML- oder NDJSON-Datei auf.\"\n        ),\n        epilog=(\n            \"Beispiele:\\\\n\"\n            \"  repo_analyzer /pfad/zum/repo -o output.json\\\\n\"\n            \"  repo_analyzer --exclude-folders build dist --include-binary --format yaml\\\\n\"\n            \"  repo_analyzer /pfad/zum/repo -o output.ndjson --format ndjson --stream\\\\n\"\n        ),\n        formatter_class=argparse.RawDescriptionHelpFormatter,\n    )\n    \n    # Pflichtargument: root_directory\n    parser.add_argument(\n        \"root_directory\",\n        type=str,\n        help=\"Das Wurzelverzeichnis des zu analysierenden Repositorys.\"\n    )\n    \n    # Optionale Argumente\n    parser.add_argument(\n        \"-o\",\n        \"--output\",\n        type=str,\n        required=True,\n        help=\"Pfad zur Ausgabedatei.\"\n    )\n    parser.add_argument(\n        \"--hash-algorithm\",\n        type=str,\n        choices=[\"md5\", \"sha1\", \"sha256\", \"sha512\"],\n        default=\"md5\",\n        help=\"Hash-Algorithmus zur Verifizierung (Standard: md5).\"\n    )\n    parser.add_argument(\n        \"--include-binary\",\n        action=\"store_true\",\n        help=\"Beinhaltet binäre Dateien und Bilddateien in der Analyse.\"\n    )\n    parser.add_argument(\n        \"--exclude-folders\",\n        nargs='*',\n        default=[],\n        help=\"Liste von Ordnernamen, die von der Analyse ausgeschlossen werden sollen.\"\n    )\n    parser.add_argument(\n        \"--exclude-files\",\n        nargs='*',\n        default=[],\n        help=\"Liste von Dateinamen, die von der Analyse ausgeschlossen werden sollen.\"\n    )\n    parser.add_argument(\n        \"--follow-symlinks\",\n        action=\"store_true\",\n        help=\"Folgt symbolischen Links während der Traversierung.\"\n    )\n    parser.add_argument(\n        \"--image-extensions\",\n        nargs='*',\n        default=[],\n        help=\"Zusätzliche Bilddateiendungen, die als binär betrachtet werden sollen.\"\n    )\n    parser.add_argument(\n        \"--exclude-patterns\",\n        nargs='*',\n        default=[],\n        help=\"Glob- oder Regex-Muster zum Ausschließen von Dateien und Ordnern.\"\n    )\n    parser.add_argument(\n        \"--threads\",\n        type=int,\n        default=None,\n        help=\"Anzahl der Threads für die parallele Verarbeitung (Standard: CPU-Kerne * 2).\"\n    )\n    parser.add_argument(\n        \"--encoding\",\n        type=str,\n        default='utf-8',\n        help=\"Standard-Encoding für Textdateien (Standard: utf-8).\"\n    )\n    parser.add_argument(\n        \"--stream\",\n        action=\"store_true\",\n        help=\"Aktiviert den Streaming-Modus für die Ausgabe (nur für JSON und NDJSON).\",\n    )\n    parser.add_argument(\n        \"--verbose\",\n        action=\"store_true\",\n        help=\"Aktiviert ausführliches Logging.\"\n    )\n    parser.add_argument(\n        \"--log-file\",\n        type=str,\n        default=None,\n        help=\"Pfad zur Logdatei.\"\n    )\n    parser.add_argument(\n        \"--no-hash\",\n        action=\"store_true\",\n        help=\"Deaktiviert die Hash-Verifizierung.\"\n    )\n    parser.add_argument(\n        \"--config\",\n        type=str,\n        default=None,\n        help=\"Pfad zur Konfigurationsdatei.\"\n    )\n    parser.add_argument(\n        \"--max-size\",\n        type=int,\n        default=None,\n        help=\"Maximale Dateigröße zum Lesen in MB (überschreibt Konfigurationsdatei).\"\
        \n    )\n    parser.add_argument(\n        \"--pool-size\",\n        type=int,\n        default=5,\n        help=\"Größe des Datenbank-Verbindungspools (Standard: 5).\"\n    )\n    parser.add_argument(\n        \"--include-summary\",\n        action=\"store_true\",\n        help=\"Fügt eine Zusammenfassung der Analyse zur Ausgabedatei hinzu.\"\n    )\n    parser.add_argument(\n        \"-f\",\n        \"--format\",\n        choices=[\"json\", \"yaml\", \"xml\", \"ndjson\"],  # NDJSON hinzugefügt\n        default=\"json\",\n        help=\"Format der Ausgabedatei (Standard: json).\",\n    )\n    parser.add_argument(\n        \"--cache-path\",\n        type=str,\n        default=get_default_cache_path(),\n        help=\"Pfad zum Cache-Verzeichnis (Standard: ~/.repo_analyzer/cache).\"\n    )\n    \n    args = parser.parse_args()\n\n    # Automatische Dateiendung hinzufügen, falls nicht vorhanden\n    if not args.output.endswith(f\".{args.format}\"):\n        args.output += f\".{args.format}\"\n\n    return args\n"
      size: 4977
      created: 1729084326.9155095
      modified: 1729356586.9600456
      permissions: 0o100666
    __init__.py:
      type: excluded
      reason: binary_or_image
  logging:
    setup.py:
      type: text
      content: "# repo_analyzer/logging/setup.py\n\nimport logging\nimport sys\nimport os\nfrom typing import Optional\n\nfrom colorama import Fore, Style\nfrom logging.handlers import RotatingFileHandler\n\n\nclass ColorFormatter(logging.Formatter):\n    \"\"\"Custom formatter to add colors to console logs based on log level.\n\n    Fügt den Konsolenlogs Farben hinzu, die dem Log-Level entsprechen.\n    \"\"\"\n\n    LEVEL_COLORS = {\n        logging.DEBUG: Fore.CYAN,\n        logging.INFO: Fore.GREEN,\n        logging.WARNING: Fore.YELLOW,\n        logging.ERROR: Fore.RED,\n        logging.CRITICAL: Fore.MAGENTA,\n    }\n\n    def __init__(self, fmt: str, datefmt: Optional[str] = None) -> None:\n        super().__init__(fmt, datefmt)\n\n    def format(self, record: logging.LogRecord) -> str:\n        color = self.LEVEL_COLORS.get(record.levelno, Fore.WHITE)\n        colored_message = f\"{color}{record.getMessage()}{Style.RESET_ALL}\"\n        original_message = record.getMessage()\n        record.message = colored_message\n        try:\n            formatted = super().format(record)\n        finally:\n            record.message = original_message  # Restore original message\n        return formatted\n\n\ndef setup_logging(\n    verbose: bool,\n    log_file: Optional[str] = None,\n    file_level: int = logging.DEBUG,\n    max_bytes: int = 5 * 1024 * 1024,\n    backup_count: int = 5,\n) -> None:\n    \"\"\"\n    Konfiguriert das Logging-Modul mit separaten Handlern für Konsole und Datei.\n\n    :param verbose: Wenn True, wird der Log-Level auf DEBUG gesetzt, sonst auf INFO.\n    :param log_file: Optionaler Pfad zur Log-Datei.\n    :param file_level: Log-Level für den Datei-Handler. Standard: DEBUG.\n    :param max_bytes: Maximale Dateigröße in Bytes für den rotierenden Datei-Handler. Standard: 5 MB.\n    :param backup_count: Anzahl der Backup-Dateien für den rotierenden Datei-Handler. Standard: 5.\n    \"\"\"\n    log_format = \"%(asctime)s - %(levelname)s - %(message)s\"\n    date_format = \"%Y-%m-%d %H:%M:%S\"\n\n    log_level = logging.DEBUG if verbose else logging.INFO\n    logger = logging.getLogger()\n    logger.setLevel(log_level)\n\n    # Entfernt alle bestehenden Handler, um doppelte Logs zu vermeiden\n    if logger.hasHandlers():\n        logger.handlers.clear()\n\n    # Format für Konsolen-Logs mit Farben\n    console_formatter = ColorFormatter(fmt=log_format, datefmt=date_format)\n\n    # Console Handler\n    console_handler = logging.StreamHandler(sys.stdout)\n    console_handler.setLevel(log_level)\n    console_handler.setFormatter(console_formatter)\n    logger.addHandler(console_handler)\n\n    if log_file:\n        try:\n            # Sicherstellen, dass das Log-Verzeichnis existiert\n            log_dir = os.path.dirname(log_file)\n            if log_dir and not os.path.exists(log_dir):\n                os.makedirs(log_dir, exist_ok=True)\n\n            # Format für Datei-Logs ohne Farben\n            file_formatter = logging.Formatter(fmt=log_format, datefmt=date_format)\n\n            # Rotating File Handler\n            file_handler = RotatingFileHandler(\n                log_file,\n                maxBytes=max_bytes,\n                backupCount=backup_count,\n                encoding=\"utf-8\",\n            )\n            file_handler.setLevel(file_level)  # Datei-Handler speichert alle Logs ab file_level\n            file_handler.setFormatter(file_formatter)\n            logger.addHandler(file_handler)\n        except (IOError, OSError) as e:\n            logger.error(f\"Failed to set up file handler: {e}\")\n"
      size: 3570
      created: 1729084327.0153632
      modified: 1729366490.5966778
      permissions: 0o100666
    __init__.py:
      type: excluded
      reason: binary_or_image
setup.py:
  type: text
  content: "from setuptools import setup, find_packages\nfrom typing import Optional, List, Dict, Any\n\nwith open('README.md', 'r', encoding='utf-8') as fh:\n    long_description = fh.read()\n\nsetup(\n    name=\"repo_analyzer\",\n    version=\"0.9.0\",\n    author=\"Lucas Richert\",\n    author_email=\"info@lucasrichert.tech\",\n    description=\"Ein Tool zur Analyse von Repository-Strukturen\",\n    long_description=long_description,\n    long_description_content_type=\"text/markdown\",\n    packages=find_packages(),\n    install_requires=[\n        \"colorama>=0.4.4\",\n        \"dicttoxml>=1.7.4\",\n        \"python-magic-bin>=0.4.14\",\n        \"PyYAML>=5.4.1\",\n        \"tqdm>=4.60.0\",\n    ],\n    classifiers=[\n        \"Programming Language :: Python :: 3\",\n        \"License :: OSI Approved :: MIT License\",\n        \"Operating System :: OS Independent\",\n    ],\n    python_requires='>=3.13',\n    entry_points={\n        \"console_scripts\": [\n            \"repo_analyzer=repo_analyzer.main:run\",\n        ],\n    },\n)\n"
  size: 1010
  created: 1729084327.0386963
  modified: 1729361575.402306
  permissions: 0o100666
requirements.txt:
  type: text
  content: 'colorama

    dicttoxml

    PyYAML

    tqdm

    python-magic-bin'
  size: 51
  created: 1729084327.0366843
  modified: 1729194521.8224583
  permissions: 0o100666
.gitignore:
  type: text
  content: '# Byte-compiled / optimized / DLL files

    __pycache__/

    *.py[cod]

    *$py.class


    # C extensions

    *.so


    # Distribution / packaging

    .Python

    build/

    develop-eggs/

    dist/

    downloads/

    eggs/

    .eggs/

    lib/

    lib64/

    parts/

    sdist/

    var/

    *.egg-info/

    .installed.cfg

    *.egg


    # Virtual environment

    venv/

    ENV/

    env/

    .venv/

    .env/

    env.bak/

    venv.bak/


    # IDEs and editors

    .vscode/

    .idea/

    *.sublime-project

    *.sublime-workspace


    # Logs

    *.log


    # Testing

    .coverage

    .cache

    nosetests.xml

    coverage.xml

    *.cover

    .hypothesis/

    .pytest_cache/

    tests/

    output.json


    # MyPy

    .mypy_cache/

    .dmypy.json

    dmypy.json


    # Environments

    .env

    .venv

    env/

    venv/

    ENV/

    env.bak/

    venv.bak/


    # Other files

    .DS_Store

    Thumbs.db


    # Jupyter Notebook checkpoints

    .ipynb_checkpoints/


    # Ignore other specific directories or files

    repo_analyzer/__pycache__/

    repo_analyzer/logging/__pycache__/

    tests/test_cache.py

    tests/test_cache.py

    tests/test_output.py

    tests/test_cache.py

    tests/test_config.py

    tests/test_logging.py

    tests/test_output.py

    tests/test_processing.py

    tests/test_traversal.py

    repo_analyzer/tests/'
  size: 1117
  created: 1729090133.091109
  modified: 1729258308.574359
  permissions: 0o100666
README.md:
  type: excluded
  reason: binary_or_image
